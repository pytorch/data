{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cf53d2-97b8-4cd0-93b2-b3d69a44c80e",
   "metadata": {},
   "source": [
    "### First we import all the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5534cad1-3d4b-48c8-899c-135651e143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import default_collate \n",
    "\n",
    "from torchdata.nodes.adapters import IterableWrapper\n",
    "from torchdata.nodes.batch import Batcher\n",
    "from torchdata.nodes.map import Mapper\n",
    "from torchdata.nodes.loader import Loader\n",
    "\n",
    "from functools import partial\n",
    "from utils import map_fn_bert, train_bert, get_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09803156-6651-4c63-aca2-246076c144e3",
   "metadata": {},
   "source": [
    "### Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e285d8-41f7-43ad-98da-02737a862d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(42).select(range(4096))\n",
    "test_dataset = dataset[\"test\"].shuffle(42).select(range(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef11f0c8-b636-47a2-9713-7ad150e9a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training = 4096, size test = 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size training = {len(train_dataset[\"text\"])}, size test = {len(test_dataset[\"text\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48ab0c-1e16-4b1d-895d-a84bfff9adc6",
   "metadata": {},
   "source": [
    "##### Let's look at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6551b76-b8f1-4156-b5ac-38b31562b0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...',\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0], train_dataset[\"label\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291509e-ced0-4cfa-a672-d45608f51692",
   "metadata": {},
   "source": [
    "##### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f85e74-0d6f-4336-8b95-f89916161e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "max_len = 512\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b6f4b-7bc2-42e4-8dd1-12a25ee379fb",
   "metadata": {},
   "source": [
    "### Next we use torchdata.nodes for defining transforms and batcher \n",
    "##### (i.e. the way in which batches will be passed during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e246ed0-e4c1-4e18-8754-666775e48356",
   "metadata": {},
   "source": [
    "#### First we make train and test `BaseNode` by using IterableWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafddc5f-cf53-4672-baf9-bc611b70a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = IterableWrapper(train_dataset)\n",
    "test_node = IterableWrapper(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027a9a1-9cc3-49bf-bc52-82418c8dbbfc",
   "metadata": {},
   "source": [
    "#### Next we use a Mapper to carry out the necessary transformations on our data before batching it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb9013c-6b9e-4604-9635-7826607b976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mapper = Mapper(train_node,partial(map_fn_bert, max_len=max_len, tokenizer=tokenizer))\n",
    "test_mapper = Mapper(test_node,partial(map_fn_bert, max_len=max_len, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bddb3-4692-4c52-990e-10b5de99d3dc",
   "metadata": {},
   "source": [
    "#### Finally we use Batcher to batch the samples together and Loader to reset the Batcher in every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada9618c-70e9-475e-9a5e-b133fc6bfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use Loader so that we do not have to reset the batcher after every epoch\n",
    "train_batcher = Loader(Batcher(train_mapper, batch_size, drop_last=True))\n",
    "test_batcher = Loader(Batcher(test_mapper, 128, drop_last=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c11fc9-c61c-49cd-af4d-3234055609ae",
   "metadata": {},
   "source": [
    "#### Let's see how a batch looks like\n",
    "\n",
    "#### We use the default collate method to collate the samples in a batch. Each batch contains batch_size = 32 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0adb6f-9b8f-41e7-af13-1d5b74e54ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2045,  2003,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  3185,  ...,     0,     0,     0],\n",
      "        [  101,  2577,  1052,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2009,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2074,  ...,     0,     0,     0],\n",
      "        [  101, 18036,  5886,  ...,  7344,  2474,   102]]) tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]) tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_batcher:\n",
    "    batch = default_collate(batch)\n",
    "    input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "    print(input_ids, attention_mask, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b13174-7e23-46ef-9cb9-d745b1e963e4",
   "metadata": {},
   "source": [
    "##### This LLM has approx 100M params, thus, training on a local machine might take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c606c1ad-74a9-47cb-957b-3060afc59006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 109 M\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())/1000000\n",
    "print(f\"Number of parameters: {num_params:.0f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c06e5-bd0e-4058-8f35-6450f8451e29",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcbf9c6-c462-4243-bd96-35a073868048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4253180552655318\n",
      "Test Loss: 0.2570107448846102, Accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "model = train_bert(model, train_batcher, test_batcher, num_epochs, batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab50e69-5e3a-46d0-8665-c6b816cefbf2",
   "metadata": {},
   "source": [
    "#### Finally our model is trained.We got an accuracy of around 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015a656-d648-44bf-ab30-fc34ce628680",
   "metadata": {},
   "source": [
    "#### Let's also test on our custom examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de24f4fa-4050-4ea6-97c5-1634b5321abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"best movie\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b02965f-469a-4a99-be37-6e9f696111de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Worst movie ever.\", model, max_len, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7dd9920-ad42-42dd-bac2-9c77674482a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"No other movie is worse than this movie.\", model, max_len, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeed1d96-4a6a-460b-b2eb-0a9871573c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"This is not very good\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c137160-efcf-4c05-a022-52d72f8e7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Will watch again\", model, max_len, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f3e3184-16d2-43c2-afd6-484de3e27601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I want to watch it again\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6cc9acf-7cf0-419d-8854-1cc9f93a9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I do not want to watch it again\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b7ade72-fd9e-4a5a-88ed-f2ea5eff577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Unwatchable\", model, max_len, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
