{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3b507c-2ad1-410d-a834-6847182de684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089f1126-7125-4274-9d71-5c949ccc7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import default_collate, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afac7d9-3d66-4195-8647-dc7034d306f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset from huggingface datasets and select the \"train\" split\n",
    "dataset = load_dataset(\"imdb\", streaming=False)\n",
    "train_dataset = dataset[\"train\"]\n",
    "# Since train_dataset is a Map-style dataset, we can setup a sampler to shuffle the data\n",
    "sampler = RandomSampler(train_dataset)\n",
    "# Use a standard bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02af5479-ee69-41d8-ab2d-bf154b84bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can set up some torchdata.nodes to create our pre-proc pipeline\n",
    "from torchdata.nodes import MapStyleWrapper, ParallelMapper, Batcher, PinMemory, Loader\n",
    "\n",
    "# All torchdata.nodes.BaseNode implementations are Iterators.\n",
    "# MapStyleWrapper creates an Iterator that combines sampler and train_dataset to create an iterator.\n",
    "#\n",
    "# Under the hood, MapStyleWrapper just does:\n",
    "# > node = IterableWrapper(sampler)\n",
    "# > node = Mapper(node, map_fn=train_dataset.__getitem__)  # You can parallelize this with ParallelMapper\n",
    "\n",
    "node = MapStyleWrapper(map_dataset=train_dataset, sampler=sampler)\n",
    "\n",
    "# Now we want to transform the raw inputs. We can just use another Mapper with\n",
    "# a custom map_fn to perform this. Using ParallelMapper allows us to use multiple\n",
    "# threads (or processes) to parallelize this work and have it run in the background\n",
    "max_len = 512\n",
    "batch_size = 32\n",
    "def bert_transform(item):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        item[\"text\"],\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "        \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        \"labels\": torch.tensor(item[\"label\"], dtype=torch.long),\n",
    "    }\n",
    "node = ParallelMapper(node, map_fn=bert_transform, num_workers=2) # output items are Dict[str, tensor]\n",
    "\n",
    "# Next we batch the inputs, and then apply a collate_fn with another Mapper\n",
    "# to stack the tensors between. We use torch.utils.data.default_collate for this\n",
    "node = Batcher(node, batch_size=batch_size) # output items are List[Dict[str, tensor]]\n",
    "node = ParallelMapper(node, map_fn=default_collate, num_workers=2) # outputs are Dict[str, tensor]\n",
    "\n",
    "# we can optionally apply pin_memory to the batches\n",
    "if torch.cuda.is_available():\n",
    "    node = PinMemory(node)\n",
    "\n",
    "# Since nodes are iterators, they need to be manually .reset() between epochs.\n",
    "# We can wrap the root node in Loader to convert it to a more conventional Iterable.\n",
    "loader = Loader(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fd54f3-62ef-47aa-a790-853cb4899f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 4283, 2000,  ...,    0,    0,    0],\n",
      "        [ 101, 2821, 2009,  ..., 7987, 1013,  102],\n",
      "        [ 101, 2023, 2034,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2228, 1997,  ..., 1045, 2228,  102],\n",
      "        [ 101, 1996, 2434,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 4149,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch\n",
    "print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f0f6c7-1238-4830-89a5-c939342e6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_bert, get_prediction_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67941fe3-f8cc-4d90-9b43-de5296387881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a similar Test DataLoader to get evaluation accuracy\n",
    "test_dataset = dataset[\"test\"]\n",
    "node = MapStyleWrapper(map_dataset=test_dataset, sampler=SequentialSampler(test_dataset))\n",
    "node = ParallelMapper(node, map_fn=bert_transform, num_workers=2) # output items are Dict[str, tensor]\n",
    "node = Batcher(node, batch_size=batch_size) # output items are List[Dict[str, tensor]]\n",
    "node = ParallelMapper(node, map_fn=default_collate, num_workers=2) # outputs are Dict[str, tensor]\n",
    "if torch.cuda.is_available():\n",
    "    node = PinMemory(node)\n",
    "test_loader = Loader(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406ebd28-5b20-43ad-bf7b-7327d9cd327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4385003423667513\n",
      "Test Loss : 0.3982642225455493, Accuracy:  0.8271\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model = train_bert(model, loader, test_loader, num_epochs=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52a0d30-b65c-4c1c-a6d9-985b742c57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "#let's check some predictions\n",
    "get_prediction_bert(\"Best movie.\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f93398-b9d2-44ee-aa98-c87be34a0da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction_bert(\"Worst movie ever.\", model, max_len, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
