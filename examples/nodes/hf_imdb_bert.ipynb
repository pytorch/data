{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5534cad1-3d4b-48c8-899c-135651e143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import default_collate \n",
    "\n",
    "\n",
    "\n",
    "from torchdata.nodes.adapters import IterableWrapper\n",
    "from torchdata.nodes.batch import Batcher\n",
    "from torchdata.nodes.map import Mapper\n",
    "\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09803156-6651-4c63-aca2-246076c144e3",
   "metadata": {},
   "source": [
    "### Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e285d8-41f7-43ad-98da-02737a862d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92d2d1e-eb10-42f2-aeef-d77efef145d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].shuffle().select(range(2048))\n",
    "test_dataset = dataset[\"test\"].shuffle().select(range(1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef11f0c8-b636-47a2-9713-7ad150e9a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training = 2048, size test = 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size training = {len(train_dataset[\"text\"])}, size test = {len(test_dataset[\"text\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48ab0c-1e16-4b1d-895d-a84bfff9adc6",
   "metadata": {},
   "source": [
    "##### let's look at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6551b76-b8f1-4156-b5ac-38b31562b0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGM were unsure of how to market Garbo when she first arrived in Hollywood. Mayer had a lot of faith in her and her appearance in \"Torrent\" justified that. She did not speak a word of English so she must have found it difficult to work, also Ricardo Cortez did not make it very easy for her.<br /><br />The torrent of the title is the river Juscar that winds through a sleepy little village in Spain. Leonora (Greta Garbo) hopes someday that her voice will bring great wealth and happiness to her struggling parents. Leonora and Don Rafael (Ricardo Cortez) are in love but he is under his mother\\'s thumb and cannot get her to consent to his marriage. Meanwhile Dona Brull (Martha Mattox) has evicted Leonora\\'s parents from their home and they send Leonora to Paris hoping to give her a chance to further her singing career. Leonora sends a note to Rafael, urging him to remember his promise and come with her. His mother is enraged and forbids him to go - so of course he caves in to her request.<br /><br />Years pass. Leonora has a new identity - she has become La Brunna, the toast of the Paris Opera. Rafael has turned out just as his mother wished - he is running for office and is courting a \"safe\" young girl, Remedios (Gertrude Olmstead) who is a \"hog\" heiress. Mack Swain plays her father. Leonora decides to visit her old home, and I agree - why hasn\\'t she helped her mother out. Her mother is still living at the family home, working as a skivvy and taking in washing. Leonora and Rafael meet but Leonora is full of ridicule. Garbo is so enchantingly beautiful, it is hard to believe that he could be happy with Remedios.<br /><br />The dam is bursting and the torrent is flooding the town. Leonora\\'s house is in the path of the raging river but when Rafael attempts to rescue her he finds she is quite safe. They then re-kindle their romance. There is a \"horizontal\" love scene in this film, very similar to the one in \"Flesh and the Devil\".<br /><br />Dona Brull goes spreading gossip about how Leonora really got her wealth and Leonora\\'s mother believes her and tells Leonora to go. Rafael meets Leonora just before she is about to tour America. Again he intends to go with her but again he lets her down. He spends so much time listening to other people destroy her reputation - \"what will she do for you but drag you down\". The irony is she has just secured a top government job for him if he comes with her. They meet again, years later - she is as fresh and vibrant as ever - he looks older than his years, bowed down by mediocrity.<br /><br />It is certainly a good film with a positive message to follow your heart.<br /><br />Lucien Littlefield does a good job as Cupido, the barber and Leonora\\'s old and faithful friend.<br /><br />Highly Recommended.',\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0], train_dataset[\"label\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291509e-ced0-4cfa-a672-d45608f51692",
   "metadata": {},
   "source": [
    "##### create train and test nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838923d2-f97f-4612-8c54-32561c95cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = IterableWrapper(train_dataset)\n",
    "test_node = IterableWrapper(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473cbb2-91ab-4ca7-bfd4-383edadc437a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f85e74-0d6f-4336-8b95-f89916161e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "max_len = 512\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b6f4b-7bc2-42e4-8dd1-12a25ee379fb",
   "metadata": {},
   "source": [
    "##### we are defining a map function that we pass in the Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bafddc5f-cf53-4672-baf9-bc611b70a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(item, max_len, tokenizer):\n",
    "    text = item[\"text\"]\n",
    "    label = item[\"label\"]\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "        \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae7948f2-0bc3-4ae7-8f9c-11c9312f28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mapper = Mapper(train_node,partial(map_fn, max_len=max_len, tokenizer=tokenizer))\n",
    "test_mapper = Mapper(test_node,partial(map_fn, max_len=max_len, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aded702-7aec-4ad0-94bc-26d375e11512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79f6655-e07c-4724-8ba1-bca397f65d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101, 15418,  2020, 12422,  1997,  2129,  2000,  3006, 11721, 15185,\n",
      "         2080,  2043,  2016,  2034,  3369,  1999,  5365,  1012, 14687,  2018,\n",
      "         1037,  2843,  1997,  4752,  1999,  2014,  1998,  2014,  3311,  1999,\n",
      "         1000, 22047,  3372,  1000, 15123,  2008,  1012,  2016,  2106,  2025,\n",
      "         3713,  1037,  2773,  1997,  2394,  2061,  2016,  2442,  2031,  2179,\n",
      "         2009,  3697,  2000,  2147,  1010,  2036, 13559,  2522, 19731,  2480,\n",
      "         2106,  2025,  2191,  2009,  2200,  3733,  2005,  2014,  1012,  1026,\n",
      "         7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996, 22047,  3372,\n",
      "         1997,  1996,  2516,  2003,  1996,  2314, 18414, 15782,  2099,  2008,\n",
      "         7266,  2083,  1037, 17056,  2210,  2352,  1999,  3577,  1012,  6506,\n",
      "         6525,  1006, 26111, 11721, 15185,  2080,  1007,  8069, 13834,  2008,\n",
      "         2014,  2376,  2097,  3288,  2307,  7177,  1998,  8404,  2000,  2014,\n",
      "         8084,  3008,  1012,  6506,  6525,  1998,  2123, 10999,  1006, 13559,\n",
      "         2522, 19731,  2480,  1007,  2024,  1999,  2293,  2021,  2002,  2003,\n",
      "         2104,  2010,  2388,  1005,  1055,  7639,  1998,  3685,  2131,  2014,\n",
      "         2000,  9619,  2000,  2010,  3510,  1012,  5564, 24260,  7987, 18083,\n",
      "         1006,  9246,  4717, 11636,  1007,  2038, 25777,  6506,  6525,  1005,\n",
      "         1055,  3008,  2013,  2037,  2188,  1998,  2027,  4604,  6506,  6525,\n",
      "         2000,  3000,  5327,  2000,  2507,  2014,  1037,  3382,  2000,  2582,\n",
      "         2014,  4823,  2476,  1012,  6506,  6525, 10255,  1037,  3602,  2000,\n",
      "        10999,  1010, 14328,  2032,  2000,  3342,  2010,  4872,  1998,  2272,\n",
      "         2007,  2014,  1012,  2010,  2388,  2003, 18835,  1998, 27206,  2015,\n",
      "         2032,  2000,  2175,  1011,  2061,  1997,  2607,  2002, 10614,  1999,\n",
      "         2000,  2014,  5227,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
      "         1013,  1028,  2086,  3413,  1012,  6506,  6525,  2038,  1037,  2047,\n",
      "         4767,  1011,  2016,  2038,  2468,  2474,  7987,  4609,  2532,  1010,\n",
      "         1996, 15174,  1997,  1996,  3000,  3850,  1012, 10999,  2038,  2357,\n",
      "         2041,  2074,  2004,  2010,  2388,  6257,  1011,  2002,  2003,  2770,\n",
      "         2005,  2436,  1998,  2003,  2457,  2075,  1037,  1000,  3647,  1000,\n",
      "         2402,  2611,  1010,  2128,  7583, 10735,  1006, 18734, 19330,  5244,\n",
      "        14565,  1007,  2040,  2003,  1037,  1000, 27589,  1000, 20020,  1012,\n",
      "        11349, 25430,  8113,  3248,  2014,  2269,  1012,  6506,  6525,  7288,\n",
      "         2000,  3942,  2014,  2214,  2188,  1010,  1998,  1045,  5993,  1011,\n",
      "         2339,  8440,  1005,  1056,  2016,  3271,  2014,  2388,  2041,  1012,\n",
      "         2014,  2388,  2003,  2145,  2542,  2012,  1996,  2155,  2188,  1010,\n",
      "         2551,  2004,  1037,  8301,  2615, 10736,  1998,  2635,  1999, 12699,\n",
      "         1012,  6506,  6525,  1998, 10999,  3113,  2021,  6506,  6525,  2003,\n",
      "         2440,  1997,  9436,  2594,  9307,  1012, 11721, 15185,  2080,  2003,\n",
      "         2061,  4372, 14856,  3436,  2135,  3376,  1010,  2009,  2003,  2524,\n",
      "         2000,  2903,  2008,  2002,  2071,  2022,  3407,  2007,  2128,  7583,\n",
      "        10735,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
      "         1996,  5477,  2003, 21305,  1998,  1996, 22047,  3372,  2003,  9451,\n",
      "         1996,  2237,  1012,  6506,  6525,  1005,  1055,  2160,  2003,  1999,\n",
      "         1996,  4130,  1997,  1996, 17559,  2314,  2021,  2043, 10999,  4740,\n",
      "         2000,  5343,  2014,  2002,  4858,  2016,  2003,  3243,  3647,  1012,\n",
      "         2027,  2059,  2128,  1011,  2785,  2571,  2037,  7472,  1012,  2045,\n",
      "         2003,  1037,  1000,  9876,  1000,  2293,  3496,  1999,  2023,  2143,\n",
      "         1010,  2200,  2714,  2000,  1996,  2028,  1999,  1000,  5771,  1998,\n",
      "         1996,  6548,  1000,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
      "         1013,  1028, 24260,  7987, 18083,  3632,  9359, 13761,  2055,  2129,\n",
      "         6506,  6525,  2428,  2288,  2014,  7177,  1998,  6506,  6525,  1005,\n",
      "         1055,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for item in train_mapper:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff86c4-e83e-4805-98be-005f79cf908c",
   "metadata": {},
   "source": [
    "##### Next we create a batcher each for training and testing. Testing batch_size we fix as 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39e47ab-8998-4d78-bb5b-203387c5a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batcher = Batcher(train_mapper, batch_size, drop_last=True)\n",
    "test_batcher = Batcher(test_mapper, 128, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c11fc9-c61c-49cd-af4d-3234055609ae",
   "metadata": {},
   "source": [
    "##### Checking how a batch looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0adb6f-9b8f-41e7-af13-1d5b74e54ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 15418,  2020,  ...,  1005,  1055,   102],\n",
      "        [  101,  2005,  6298,  ...,     0,     0,     0],\n",
      "        [  101,  4462,  5822,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2028,  1997,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  ...,     0,     0,     0],\n",
      "        [  101, 15390,  2039,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]) tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_batcher:\n",
    "    batch = default_collate(batch)\n",
    "    input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "    print(input_ids, attention_mask, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b13174-7e23-46ef-9cb9-d745b1e963e4",
   "metadata": {},
   "source": [
    "##### This LLM has approx 100M params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c606c1ad-74a9-47cb-957b-3060afc59006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 0.109483778 B\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())/1000000000\n",
    "print(f\"Number of parameters: {num_params} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fcbf9c6-c462-4243-bd96-35a073868048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 32 samples.\n",
      "Trained on 64 samples.\n",
      "Trained on 96 samples.\n",
      "Trained on 128 samples.\n",
      "Trained on 160 samples.\n",
      "Trained on 192 samples.\n",
      "Trained on 224 samples.\n",
      "Trained on 256 samples.\n",
      "Trained on 288 samples.\n",
      "Trained on 320 samples.\n",
      "Trained on 352 samples.\n",
      "Trained on 384 samples.\n",
      "Trained on 416 samples.\n",
      "Trained on 448 samples.\n",
      "Trained on 480 samples.\n",
      "Trained on 512 samples.\n",
      "Trained on 544 samples.\n",
      "Trained on 576 samples.\n",
      "Trained on 608 samples.\n",
      "Trained on 640 samples.\n",
      "Trained on 672 samples.\n",
      "Trained on 704 samples.\n",
      "Trained on 736 samples.\n",
      "Trained on 768 samples.\n",
      "Trained on 800 samples.\n",
      "Trained on 832 samples.\n",
      "Trained on 864 samples.\n",
      "Trained on 896 samples.\n",
      "Trained on 928 samples.\n",
      "Trained on 960 samples.\n",
      "Trained on 992 samples.\n",
      "Trained on 1024 samples.\n",
      "Trained on 1056 samples.\n",
      "Trained on 1088 samples.\n",
      "Trained on 1120 samples.\n",
      "Trained on 1152 samples.\n",
      "Trained on 1184 samples.\n",
      "Trained on 1216 samples.\n",
      "Trained on 1248 samples.\n",
      "Trained on 1280 samples.\n",
      "Trained on 1312 samples.\n",
      "Trained on 1344 samples.\n",
      "Trained on 1376 samples.\n",
      "Trained on 1408 samples.\n",
      "Trained on 1440 samples.\n",
      "Trained on 1472 samples.\n",
      "Trained on 1504 samples.\n",
      "Trained on 1536 samples.\n",
      "Trained on 1568 samples.\n",
      "Trained on 1600 samples.\n",
      "Trained on 1632 samples.\n",
      "Trained on 1664 samples.\n",
      "Trained on 1696 samples.\n",
      "Trained on 1728 samples.\n",
      "Trained on 1760 samples.\n",
      "Trained on 1792 samples.\n",
      "Trained on 1824 samples.\n",
      "Trained on 1856 samples.\n",
      "Trained on 1888 samples.\n",
      "Trained on 1920 samples.\n",
      "Trained on 1952 samples.\n",
      "Trained on 1984 samples.\n",
      "Trained on 2016 samples.\n",
      "Trained on 2048 samples.\n",
      "Epoch 1, Loss: 0.5494920999284775\n",
      "Tested 128 samples\n",
      "Tested 256 samples\n",
      "Tested 384 samples\n",
      "Tested 512 samples\n",
      "Tested 640 samples\n",
      "Tested 768 samples\n",
      "Tested 896 samples\n",
      "Tested 1024 samples\n",
      "Test Loss: 0.33710693940520287, Accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_batcher.reset()\n",
    "    test_batcher.reset()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for num_loop, batch in enumerate(train_batcher):\n",
    "\n",
    "        batch = default_collate(batch)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Trained on {(num_loop+1)*batch_size} samples.\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / num_loop}\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        num_samples_tested=0\n",
    "        num_loops=0\n",
    "        for batch in test_batcher:\n",
    "            \n",
    "            batch = default_collate(batch)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            num_samples_tested+=128\n",
    "            num_loops+=1\n",
    "            print(f\"Tested {num_samples_tested} samples\")\n",
    "    accuracy = correct / num_samples_tested\n",
    "    print(f\"Test Loss: {test_loss / num_loops}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b06427-be28-4cf5-802f-4cae785d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Let's also test on our custom examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98f84489-6c19-41d0-bf24-ba8cbf864d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(review, model, max_len, tokenizer):\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].flatten()\n",
    "    input_ids = input_ids.unsqueeze(0) \n",
    "    attention_mask = encoding[\"attention_mask\"].flatten()\n",
    "    attention_mask = attention_mask.unsqueeze(0) \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits)\n",
    "        if predicted_class==0:\n",
    "            print(\"Negative\")\n",
    "        else:\n",
    "            print(\"Positive\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de24f4fa-4050-4ea6-97c5-1634b5321abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"best movie\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7dd9920-ad42-42dd-bac2-9c77674482a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"No other movie is worse than this movie.\", model, max_len, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeed1d96-4a6a-460b-b2eb-0a9871573c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"This is not very good\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c137160-efcf-4c05-a022-52d72f8e7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Will watch again\", model, max_len, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f3e3184-16d2-43c2-afd6-484de3e27601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I want to watch it again\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6cc9acf-7cf0-419d-8854-1cc9f93a9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I do not want to watch it again\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b7ade72-fd9e-4a5a-88ed-f2ea5eff577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Unwatchable\", model, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f19f9e-80ae-47d9-a622-6b96161e1bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
