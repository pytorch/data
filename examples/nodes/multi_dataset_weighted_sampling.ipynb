{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a14c63-a085-493f-8db9-6af3e1d744b5",
   "metadata": {},
   "source": [
    "### In this notebook, we will explore the usage of MultiNodeWeightedSampler in torchdata.nodes\n",
    "\n",
    "#### MultiNodeWeightedSampler allows us to sample with a probability from multiple datsets\n",
    "#### We will make three datasets, and then see how does the composition of the batch depend on the weights defined in the MultiNodeWeightedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e552c06-cd2f-4325-b07d-e0386bca4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from torchdata.nodes import MapStyleWrapper, Batcher,  Loader, Mapper, MultiNodeWeightedSampler\n",
    "\n",
    "# defining a simple map_fn as a place holder example\n",
    "def map_fn(item):\n",
    "    return {\"x\":item}\n",
    "\n",
    "# In this function, we create a dictionary of datasets, that can be passed to the MultiNodeWeightedSampler\n",
    "# Each dataset contains just one value, `length` number of times\n",
    "\n",
    "def get_datasets(num_datasets, length=100000):\n",
    "    \"\"\"\n",
    "    Create a dictionary of datasets with simple transformations.\n",
    "    Args:\n",
    "        num_datasets (int): Number of datasets to create.\n",
    "        length (int, optional): Length of each dataset. Defaults to 10000.\n",
    "    Returns:\n",
    "        dict: Dictionary of datasets with simple transformations.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    for i in range(num_datasets):\n",
    "        data = [i] * length # We first create a list, all elements are have value = i\n",
    "        sampler = SequentialSampler(data) # We can use a SequentialSampler or a RandomSampler if we want to shuffle the dataset\n",
    "        # Next we create a BaseNode, by passing our dataset, and sampler\n",
    "        node = MapStyleWrapper(map_dataset=data, sampler=sampler) \n",
    "        # Next, we apply our simple transformation of changing the type of each element\n",
    "        datasets[f\"ds{i}\"] = Mapper(node, map_fn=map_fn)\n",
    "    return datasets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df2694c-7c3c-405c-a683-61b46b730597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a dictionary of three datasets\n",
    "num_datasets = 3\n",
    "datasets = get_datasets(num_datasets)\n",
    "\n",
    "# Next, we have to define weights for sampling from a particular dataset\n",
    "# Make sure that the weights dict has the same keys as the datasets\n",
    "weights = {\"ds0\":0.5, \"ds1\":0.25, \"ds2\":0.25}\n",
    "\n",
    "# Finally we instatiate the MultiNodeWeightedSampler to sample from our datasets\n",
    "multi_node_sampler = MultiNodeWeightedSampler(datasets, weights)\n",
    "\n",
    "# We can use the Batcher functionality to create batches of `batch_size`\n",
    "multi_node_batcher = Batcher(multi_node_sampler, batch_size = 1000)\n",
    "\n",
    "# Since nodes are iterators, they need to be manually .reset() between epochs.\n",
    "# We can wrap the root node in Loader to convert it to a more conventional Iterable.\n",
    "train_loader = Loader(multi_node_batcher)\n",
    "\n",
    "# This train loader can be used to provide batches during training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ae6543-7797-4638-80d9-7ead84f08eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have written a function to find the composition of each batch,\n",
    "# and see if the batch composition follows our given weights\n",
    "from collections import Counter\n",
    "def compute_dataset_fraction(num_datasets, batch):\n",
    "\n",
    "    total_length = len(batch)\n",
    "    results = [item[\"x\"] for item in batch]\n",
    "    counts = Counter(results)\n",
    "    fractions = {}\n",
    "    for key, value in counts.items():\n",
    "        fractions[f\"ds{int(key)}\"] = value/total_length\n",
    "    for i in range(num_datasets):\n",
    "        print(f\"The fraction of ds{i} is = \", fractions[f\"ds{i}\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a5a929-f4ad-44bf-b494-2fb47df5397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of ds0 is =  0.511\n",
      "The fraction of ds1 is =  0.244\n",
      "The fraction of ds2 is =  0.245\n",
      "The original weights were {'ds0': 0.5, 'ds1': 0.25, 'ds2': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# let's go through the batches, and compute the fraction of each dataset in that batch\n",
    "for batch in train_loader:\n",
    "\n",
    "    compute_dataset_fraction(num_datasets, batch)\n",
    "    print(\"The original weights were\", weights)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb99df-7c62-47a4-b6c0-233a85f2e453",
   "metadata": {},
   "source": [
    "#### Since picking items from the datasets according to weights is a stochastic process, the fraction of datasets in the batch is close to provided weights, but not exactly equal.\n",
    "#### If we increase the batch_size, the fractions will asymptotically reach the provided weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca907d8b-7a8f-4322-88f2-c9d4359a88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of ds0 is =  0.50148\n",
      "The fraction of ds1 is =  0.24844\n",
      "The fraction of ds2 is =  0.25008\n",
      "The original weights were {'ds0': 0.5, 'ds1': 0.25, 'ds2': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Let's try a bigger batch size\n",
    "multi_node_batcher = Batcher(multi_node_sampler, batch_size = 100000)\n",
    "\n",
    "train_loader = Loader(multi_node_batcher)\n",
    "\n",
    "# This time we get fractions much closer to our provided weights\n",
    "for batch in train_loader:\n",
    "\n",
    "    compute_dataset_fraction(num_datasets, batch)\n",
    "    print(\"The original weights were\", weights)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
