{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd056f626f2aa31d1996176a2034a75bfff22e4b2d79ede5b92d2ea330b3279997c",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TorchArrow in 10 minutes\n",
    "\n",
    "TorchArrow is a Python DataFrame library built on the Apache Arrow columnar memory format and leveraging the Velox vectorized engine for loading, filtering, mapping, joining, aggregating, and otherwise manipulating tabular data on CPUs.\n",
    "\n",
    "TorchArrow allows mostly zero copy interop with Numpy, Pandas, PyArrow, CuDf and of course PyTorch.\n",
    "In fact, it is the integration with PyTorch which has triggered the development of TorchArrow. \n",
    "So TorchArrow understands Tensors natively.  \n",
    "\n",
    "(Remark. In case the following looks familiar, it is with gratitude that portions of this tutorial were borrowed and adapted from the 10 Minutes to Pandas (and CuDF) tutorial.)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "source": [
    "The TorchArrow library consists of 3 parts: \n",
    "\n",
    "  * *DTypes* define *Schema*, *Fields*, primitive and composite *Types*. \n",
    "  * *Columns* defines sequences of strongly typed data with vectorized operations.\n",
    "  * *Dataframes*  are sequences of named and typed columns of same length with relational operations.  \n",
    "\n",
    "Let's get started..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torcharrow as T\n",
    "ta = T.Session()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "## Constructing data: Columns\n",
    "\n",
    "### From Pandas to TorchArrow\n",
    "To start let's create a Panda series and a TorchArrow column and compare them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    2.0\n",
       "2    NaN\n",
       "3    4.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "pd.Series([1,2,None,4])"
   ]
  },
  {
   "source": [
    "In Pandas each Series has an index, here depicted as the first column. Note also that the inferred type is float and not int, since in Pandas None implicitly  promotes an int list to a float series."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "TorchArrow has a much more precise type system:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  1\n",
       "1  2\n",
       "2  None\n",
       "3  4\n",
       "dtype: Int64(nullable=True), length: 4, null_count: 1"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "s = ta.Column([1,2,None,4])\n",
    "s"
   ]
  },
  {
   "source": [
    "TorchArrow infers that that the type is `Int64(nullable=True)` which required that the vectors is represented internally via two arrays, its data and validity bit mask (the current implementation uses one byte for each bit). We can make the internal representation explicit by looking at the underlying representation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  data    mask\n------  ------\n     1       0\n     2       0\n     0       1\n     4       0\n"
     ]
    }
   ],
   "source": [
    " from tabulate import tabulate\n",
    " \n",
    " print(tabulate([(d,m) for d,m in zip(s._data, s._mask)], headers = [\"data\", \"mask\"]))"
   ]
  },
  {
   "source": [
    "Of course, we can always get lots of more information  from a column:  the `length`, `count`, `null_count` determine the total number, the number of non-null, and the number of nulls, respectivly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 3, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(s), s.count(), s.null_count()\n"
   ]
  },
  {
   "source": [
    "TorchArrow supports (almost all of Arrow types), including arbitrarily nested structs, maps, lists, and fixed size lists. Here is a column of a list of strings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  ['hello', 'world']\n",
       "1  ['how', 'are', 'you']\n",
       "dtype: List_(string), length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "sf = ta.Column([ [\"hello\", \"world\"], [\"how\", \"are\", \"you\"] ], dtype=T.List_(T.string))\n",
    "sf"
   ]
  },
  {
   "source": [
    "And here is a column of average climate data, one map per continent, with city as key and yearly average min and max temperature:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  {'helsinki': [-1.3, 21.5], 'moskow': [-4.0, 24.3]}\n",
       "1  {'algiers': [11.2, 25.0, 2.0], 'kinshasa': [22.2, 26.8]}\n",
       "dtype: Map(string, List_(float64)), length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "mf = ta.Column([ \n",
    "    {'helsinki': [-1.3, 21.5], 'moskow': [-4.0,24.3]}, \n",
    "    {'algiers':[11.2, 25,2], 'kinshasa':[22.2,26.8]}\n",
    "    ])\n",
    "mf"
   ]
  },
  {
   "source": [
    "### Append and concat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Columns are immutable (or in more detail: the public API defines columns as being immutable). Use `append` to add a list of values or `concat` to combine a list of columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  ['hello', 'world']\n",
       "1  ['how', 'are', 'you']\n",
       "2  ['I', 'am', 'fine']\n",
       "dtype: List_(string), length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "sf = sf.append([[\"I\", \"am\", \"fine\"]])\n",
    "sf"
   ]
  },
  {
   "source": [
    "\n",
    "## Constructing data: Dataframes\n",
    "\n",
    "A Dataframe is just a set of named and strongly typed columns of equal length:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c\n",
       "-------  ---  ---  ---\n",
       "      0    0    6    0\n",
       "      1    1    5    1\n",
       "      2    2    4    2\n",
       "      3    3    3    3\n",
       "      4    4    2    4\n",
       "      5    5    1    5\n",
       "      6    6    0    6\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64)]), count: 7, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df = ta.DataFrame({'a': list(range(7)),\n",
    "                     'b': list(reversed(range(7))),\n",
    "                     'c': list(range(7))\n",
    "                    })\n",
    "df"
   ]
  },
  {
   "source": [
    "To access a dataframes columns write:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "source": [
    "Dataframes are also immutable, except you can always add a new column, provided its name hasen't been used. The column is appended to the set of existing columns at the end."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    0    6    0   99\n",
       "      1    1    5    1  100\n",
       "      2    2    4    2  101\n",
       "      3    3    3    3  102\n",
       "      4    4    2    4  103\n",
       "      5    5    1    5  104\n",
       "      6    6    0    6  105\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 7, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df['d'] = ta.Column(list(range(99, 99+7)))\n",
    "df"
   ]
  },
  {
   "source": [
    "Dataframes can be nested. Here is a Dataframe having sub-dataframes. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a  b\n",
       "-------  ---  ---------\n",
       "      0    1  (11, 111)\n",
       "      1    2  (22, 222)\n",
       "      2    3  (33, 333)\n",
       "dtype: Struct([Field('a', int64), Field('b', Struct([Field('b1', int64), Field('b2', int64)]))]), count: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "\n",
    "df_inner = ta.DataFrame({'b1': [11, 22, 33], 'b2':[111,222,333]})\n",
    "df_outer = ta.DataFrame({'a': [1, 2, 3], 'b':df_inner})\n",
    "df_outer"
   ]
  },
  {
   "source": [
    "We can not only add columns to dataframes, we can append rows, too. A row of a dataframe is expressed as a tuple. So a row of a nested dataframe is represented by a nested tuple. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Currenetly failing; chcek why...\n",
    "# df_outer = df_outer.append([(4,(44,444))])\n",
    "# df_outer"
   ]
  },
  {
   "source": [
    "## Interop\n",
    "\n",
    "Take a Pandas dataframe and move it zero copy (if possible) to TorchArrow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b\n",
       "-------  ---  ---\n",
       "      0    0  0.1\n",
       "      1    1  0.2\n",
       "      2    2\n",
       "      3    3  0.3\n",
       "dtype: Struct([Field('a', int64), Field('b', float64)]), count: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\n",
    "pdf = pd.DataFrame({'a': [0, 1, 2, 3],'b': [0.1, 0.2, None, 0.3]})\n",
    "gdf = T.from_pandas_dataframe(pdf, session= ta)\n",
    "gdf"
   ]
  },
  {
   "source": [
    "And bring it back to Pandas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   a    b\n",
       "0  0  0.1\n",
       "1  1  0.2\n",
       "2  2  NaN\n",
       "3  3  0.3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "gdf.to_pandas()"
   ]
  },
  {
   "source": [
    "The same works for arrow, too. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "a: int64\n",
       "b: double"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "T.from_arrow_table(pa.table({'a': [0, 1, 2, 3],'b': [0.1, 0.2, None, 0.3]})).to_arrow()"
   ]
  },
  {
   "source": [
    "## Viewing (sorted) data\n",
    "\n",
    "Take the (head of) the top n rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    0    6    0   99\n",
       "      1    1    5    1  100\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "source": [
    "Or return the last n rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    6    0    6  105\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 1, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df.tail(1)\n"
   ]
  },
  {
   "source": [
    "or sort the values before hand."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    0    6    0   99\n",
       "      1    1    5    1  100\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df.sort(by=['c', 'b']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Sorting can be controlled not only by which columns to sort on, but also whether to sort ascending or descending, and how to deal with nulls, are they listed first or last.  \n",
    "\n",
    "## Selection using Indices\n",
    "\n",
    "Torcharrow supports two indices:\n",
    " - Integer indices select rows\n",
    " - String indices select columns\n",
    "\n",
    "So projecting a single column of a dataframe is simply"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "5  5\n",
       "6  6\n",
       "dtype: int64, length: 7, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df['a']"
   ]
  },
  {
   "source": [
    "Selecting a single row uses an integer index. (In Torcharrow everything is zero-based.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 5, 1, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "df[1]"
   ]
  },
  {
   "source": [
    "Selecting a slice keeps the type alive. Here we slice rows:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    2    4    2  101\n",
       "      1    4    2    4  103\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df[2:6:2]"
   ]
  },
  {
   "source": [
    "But you can also slice columns. The below return all columns after and including 'c'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    c    d\n",
       "-------  ---  ---\n",
       "      0    0   99\n",
       "      1    1  100\n",
       "      2    2  101\n",
       "      3    3  102\n",
       "      4    4  103\n",
       "      5    5  104\n",
       "      6    6  105\n",
       "dtype: Struct([Field('c', int64), Field('d', int64)]), count: 7, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df['c':]"
   ]
  },
  {
   "source": [
    "You can even access columns by position. Simply pass the columns index as a string. So above `df['c':]` is the same as `df['2':]`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Torcharrow follows the normal Python semantics for slices: that is a slice interval is closed on the left and open on the right."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Selection by Condition\n",
    "\n",
    "Selection of a column or dataframe *c* by a condition takes a boolean column *b* of the same length as *c*. If the *i*th row in *b* is true, *c*'s *i*th row is included in the result otherwise it is dropped. Below expression selects the first row, since it is true, and drops all remaining rows, since they are false.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    0    6    0   99\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 1, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df[[True] + [False] * (len(df)-1)]"
   ]
  },
  {
   "source": [
    "Conditional expressions over vectors return boolean vectors. Conditionals are thus the usual way to write filters. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "b = df['a'] > 4\n",
    "df[b]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    5    1    5  104\n",
       "      1    6    0    6  105\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "source": [
    "Torcharrow supports all the usual predicates, like <,==,!=>,>=,<= as well as _in_. The later is denoted by `isin`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b    c    d\n",
       "-------  ---  ---  ---  ---\n",
       "      0    5    1    5  104\n",
       "dtype: Struct([Field('a', int64), Field('b', int64), Field('c', int64), Field('d', int64)]), count: 1, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "df[df['a'].isin([5])]"
   ]
  },
  {
   "source": [
    "## Missing data\n",
    " Missing data can be filled in via the `fillna` method "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2  999\n",
       "3    4\n",
       "dtype: int64, length: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "t = s.fillna(999)\n",
    "t"
   ]
  },
  {
   "source": [
    "Alternatively data that has null data can be dropped:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  1\n",
       "1  2\n",
       "2  4\n",
       "dtype: int64, length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "s.dropna()"
   ]
  },
  {
   "source": [
    "## Operators\n",
    "Columns and dataframes support all of Python's usual binary operators, like  ==,!=,<=,<,>,>= for equality  and comparison,  +,-,*,,/.//,** for performing arithmetic and &,|,~ for conjunction, disjunction and negation. \n",
    "\n",
    "The semantics of each operator is given by lifting their scalar operation to vectors and dataframes. So given for instance a scalar comparison operator, in TorchArrow a scalar can be compared to each item in a column, two columns can be compared pointwise, a column can be compared to each column of a dataframe, and two dataframes can be compared by comparing each of their respective columns. \n",
    "\n",
    "Here are some example expressions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0   0\n",
       "1   0\n",
       "2   2\n",
       "3   6\n",
       "4  12\n",
       "dtype: int64, length: 5, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "u = ta.Column(list(range(5)))\n",
    "v = -u\n",
    "w = v+1\n",
    "v*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    a    b\n",
       "-------  ---  ---\n",
       "      0    1    1\n",
       "      1    1    0\n",
       "      2    1    0\n",
       "      3    1    0\n",
       "      4    1    0\n",
       "dtype: Struct([Field('a', boolean), Field('b', boolean)]), count: 5, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "uv = ta.DataFrame({'a': u, 'b': v})\n",
    "uu = ta.DataFrame({'a': u, 'b': u})\n",
    "(uv==uu)"
   ]
  },
  {
   "source": [
    "## Null strictness\n",
    "\n",
    "The default behavior of torcharrow operators and functions is that *if any argument is null then the result is null*. For instance:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  12\n",
       "1  None\n",
       "2  None\n",
       "dtype: Int64(nullable=True), length: 3, null_count: 2"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "u = ta.Column([1,None,3])\n",
    "v = ta.Column([11,None, None])\n",
    "u+v"
   ]
  },
  {
   "source": [
    "If null strictness does not work for your code you could call first `fillna` to provide a value that is used instead of null. \n",
    "\n",
    "NOTE: THIS IS CURRENTLY DISABLED. But since you might need different values for different operators, all operators and functions in torcharrow provide an optional parameter for a fillna value. So if we wanted to use 7 instead of null, we could write (where `add` is the function name for the operator `+`): END OF NOTE\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u.add(v, fill_value=7)"
   ]
  },
  {
   "source": [
    "## Numerical columns and descriptive statistics\n",
    "Numerical columns also support lifted operations, for `abs`, `ceil`, `floor`, `round`. Even more excited might be to use their aggregation operators like `count`, `sum`, `prod`, `min`, `max`, or descriptive statistics like `std`, `mean`, `median`, and `mode`. Here is an example ensemble:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 999, 1006, 251.5)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "(t.min(), t.max(), t.sum(), t.mean())"
   ]
  },
  {
   "source": [
    "The `describe` method puts this nicely together: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  statistic      value\n",
       "-------  -----------  -------\n",
       "      0  count          4\n",
       "      1  mean         251.5\n",
       "      2  std          498.335\n",
       "      3  min            1\n",
       "      4  25%            1.5\n",
       "      5  50%            3\n",
       "      6  75%          501.5\n",
       "      7  max          999\n",
       "dtype: Struct([Field('statistic', string), Field('value', float64)]), count: 8, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "t.describe()"
   ]
  },
  {
   "source": [
    "Sum, prod, min and max are also available as accumulating operators called `cusum`, `cuprod`, etc. \n",
    "\n",
    "Boolean vectors are very similar to numerial vector. They offer the aggregation operators `any` and `all`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## String, list and map methods\n",
    "Torcharrow provides all of Python's string, list and map processing methods, just lifted to work over columns. Like in Pandas they are all accessible via the `str`, `list` and `map` property, respectivly.\n",
    "\n",
    "### Strings\n",
    "Let's capitalize a column of strings.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  'What a wonderful world!'\n",
       "1  'Really?'\n",
       "dtype: string, length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "s = ta.Column(['what a wonderful world!', 'really?'])\n",
    "s.str.capitalize()"
   ]
  },
  {
   "source": [
    "Split is more involved. We have to decide wether a string gets split into a list of strings or into spread over a set of columns. So split gets an extra parameter called expand: the default is that expand=False, oin which case split return a list column, if expand=True split return a list of columns as a dataframe:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  ['what', 'a', 'wonderful', 'world!']\n",
       "1  ['really?']\n",
       "dtype: List_(string), length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "ss= s.str.split(sep=' ')\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  0        1    2\n",
       "-------  -------  ---  ----------------\n",
       "      0  what     a    wonderful world!\n",
       "      1  really?\n",
       "dtype: Struct([Field('0', String(nullable=True)), Field('1', String(nullable=True)), Field('2', String(nullable=True))]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "cs =  s.str.split(sep=' ', expand = True, maxsplit = 2)\n",
    "cs"
   ]
  },
  {
   "source": [
    "### Lists\n",
    "\n",
    "To operate on a list column use the usual pure list operations, like `len(gth)`, `slice`, `index` and `count`, etc. But there are a couple of additional operations. \n",
    "\n",
    "For instance to invert the result of a string split operation a list of string column also offers a join operation. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  'what-a-wonderful-world!'\n",
       "1  'really?'\n",
       "dtype: string, length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "ss.list.join(sep='-')"
   ]
  },
  {
   "source": [
    "In addition lists provide `filter`, `map`, `flatmap` and `reduce` operators, which we will discuss as in more details in functional tools.\n",
    "\n",
    "### Maps\n",
    "\n",
    "Column of type map provide the usual map operations like `len(gth)`, `[.]`, `keys` and `values`. Keys and values both return a list column. Key and value columns can be reassembled by calling `mapsto`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "mf.map.keys()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  ['helsinki', 'moskow']\n",
       "1  ['algiers', 'kinshasa']\n",
       "dtype: List_(string), length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "source": [
    "## Relational tools: Where, select, groupby, join, etc.\n",
    " \n",
    "Torcharrow will soon support all relational operators on dataframes. The following sections discuss what exists today.\n",
    "\n",
    "### Where\n",
    "The simplest operator is `df.where(p)` which is just another way of writing `df[p]`. (Note: TorchArrow's `where`  != Pandas' `where`, the latter is a vectorized if-then-else which we call in Torcharrow `ite`.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      B    C\n",
       "-------  ---  ---  ---\n",
       "      0  a      3   12\n",
       "      1  b      4   13\n",
       "dtype: Struct([Field('A', string), Field('B', int64), Field('C', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "xf = ta.DataFrame({\n",
    "    'A':['a', 'b', 'a', 'b'], \n",
    "    'B': [1, 2, 3, 4], \n",
    "    'C': [10,11,12,13]})\n",
    "\n",
    "xf.where(xf['B']>2)"
   ]
  },
  {
   "source": [
    "Note that in `df.where` the predicate `df['B']>2` refers to self, i.e. `df`. To access self in an expression torchArrow introduces the special name `me`. That is, we can also write:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      B    C\n",
       "-------  ---  ---  ---\n",
       "      0  a      3   12\n",
       "      1  b      4   13\n",
       "dtype: Struct([Field('A', string), Field('B', int64), Field('C', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from torcharrow import me\n",
    "xf.where(me['B']>2)\n"
   ]
  },
  {
   "source": [
    "### Select\n",
    "\n",
    "Select is SQL's standard way to define a new set of columns. We use *positional args to keep columns and kwargs to give new bindings. Here is a typical example that keeps all of df's columns but adds column 'D').\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      B    C    D\n",
       "-------  ---  ---  ---  ---\n",
       "      0  a      1   10   11\n",
       "      1  b      2   11   13\n",
       "      2  a      3   12   15\n",
       "      3  b      4   13   17\n",
       "dtype: Struct([Field('A', string), Field('B', int64), Field('C', int64), Field('D', int64)]), count: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "\n",
    "xf.select(*xf.columns, D=me['B']+me['C'])"
   ]
  },
  {
   "source": [
    "The shorform of `*xf.columns` is '\\*', so `xf.select('*', D=me['B']+me['C'])` does the same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Grouping\n",
    "\n",
    "Like pandas, torcharrow supports the Split-Apply-Combine groupby paradigm. Let's see a couple of examples: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      B.sum\n",
       "-------  ---  -------\n",
       "      0  a          4\n",
       "      1  b          6\n",
       "dtype: Struct([Field('A', string), Field('B.sum', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "df = ta.DataFrame({'A': ['a', 'b', 'a', 'b'], 'B': [1, 2, 3, 4]})\n",
    "\n",
    "#group by A\n",
    "grouped = df.groupby(['A'])\n",
    "\n",
    "# apply sum on each of B's grouped column to create a new column\n",
    "grouped_sum = grouped['B'].sum()\n",
    "\n",
    "#combine a new dataframe from old and new columns\n",
    "res = ta.DataFrame()\n",
    "res['A']= grouped['A']\n",
    "res['B.sum']= grouped_sum\n",
    "\n",
    "res"
   ]
  },
  {
   "source": [
    "The same can be written as a one liner:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      B.sum\n",
       "-------  ---  -------\n",
       "      0  a          4\n",
       "      1  b          6\n",
       "dtype: Struct([Field('A', string), Field('B.sum', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df.groupby(['A']).sum()\n"
   ]
  },
  {
   "source": [
    "Of course, you can group by more than one column, e.g. `df.groupby(['A','B'])`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If you wanted to apply a whole set of functions on different parts of the dataframe use groupby followed by select on grouped data. (TorchArow also supports Pandas `agg` and `aggregate` function, which are not sampled here.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index  A      b_sum    c_count\n",
       "-------  ---  -------  ---------\n",
       "      0  a          4          2\n",
       "      1  b          6          2\n",
       "dtype: Struct([Field('A', string), Field('b_sum', int64), Field('c_count', int64)]), count: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "df = ta.DataFrame({\n",
    "    'A':['a', 'b', 'a', 'b'], \n",
    "    'B': [1, 2, 3, 4], \n",
    "    'C': [10,11,12,13]})\n",
    "df.groupby(['A']).select(b_sum=me['B'].sum(), c_count=me['C'].count())"
   ]
  },
  {
   "source": [
    "All aggregation functions (`min`, `max`, `any`, `all`, `sum`, `prod`, `count`, etc)  are available. In none of these work one can use `reduce`. \n",
    "\n",
    "Finally to see what data groups contain iterate over them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('a',)\n   self._fromdata({'B':Column([1, 3], id = c159), id = c160})\n('b',)\n   self._fromdata({'B':Column([2, 4], id = c161), id = c162})\n"
     ]
    }
   ],
   "source": [
    "for g, df in grouped:\n",
    "    print(g)\n",
    "    print(\"  \",df)"
   ]
  },
  {
   "source": [
    "### Join -- TODO\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_a = ta.DataFrame()\n",
    "# df_a['key'] = ['a', 'b', 'c', 'd', 'e']\n",
    "# df_a['vals_a'] = [float(i + 10) for i in range(5)]\n",
    "\n",
    "# df_b = ta.DataFrame()\n",
    "# df_b['key'] = ['a', 'c', 'e']\n",
    "# df_b['vals_b'] = [float(i+100) for i in range(3)]\n",
    "\n",
    "# merged = df_a.join(df_b, on=['key'], how='left')\n",
    "# merged"
   ]
  },
  {
   "source": [
    "### Transpose -- TODO\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ta.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.transpose() -TODO"
   ]
  },
  {
   "source": [
    "## User defined functions and functional tools:  map, filter, reduce\n",
    "\n",
    "Column and dataframe piplines support map/reduce style programming as well. We first explore column oriented operations.\n",
    "\n",
    "###  Map and its variations\n",
    "\n",
    "`map` maps values of a column according to input correspondence. The input correspondance can be given as a mapping or as a (user-defined-) function (UDF). If the mapping is a dict, then non mapped values become null.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  111\n",
       "1  None\n",
       "2  None\n",
       "3  None\n",
       "dtype: Int64(nullable=True), length: 4, null_count: 3"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "ta.Column([1,2,None,4]).map({1:111})"
   ]
  },
  {
   "source": [
    "If the mapping is a defaultdict, all values will be mapped as decribed by the default dict."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  111\n",
       "1   -1\n",
       "2   -1\n",
       "3   -1\n",
       "dtype: Int64(nullable=True), length: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "ta.Column([1,2,None,4]).map(defaultdict(lambda: -1, {1:111}))"
   ]
  },
  {
   "source": [
    "If the mapping is a function, then it will be applied on all values (including null), unless na_action is `'ignore'`, in which case, null values are passed through."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  11\n",
       "1  12\n",
       "2  None\n",
       "3  14\n",
       "dtype: Int64(nullable=True), length: 4, null_count: 1"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "def add_ten(num):\n",
    "    return num + 10\n",
    "\n",
    "ta.Column([1,2,None,4]).map(add_ten, na_action='ignore')"
   ]
  },
  {
   "source": [
    "Note that `.map(add_ten, na_action=None)` would fail with a type error since `addten` is not defined for `None`/null. So if we wanted to pass null to `add_ten` we would have to prepare for it, maybe like so:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  11\n",
       "1  12\n",
       "2   0\n",
       "3  14\n",
       "dtype: Int64(nullable=True), length: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "def add_ten_or_0(num):\n",
    "    return 0 if num is None else num + 10\n",
    "    \n",
    "ta.Column([1,2,None,4]).map(add_ten_or_0, na_action= None)"
   ]
  },
  {
   "source": [
    "**Mapping to different types.** If `map` returns a column type that is different from the input column type, then `map` has to specify the returned column type. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  '1'\n",
       "1  '2'\n",
       "2  '3'\n",
       "3  '4'\n",
       "dtype: string, length: 4, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "ta.Column([1,2,3,4]).map(str, dtype=T.string)"
   ]
  },
  {
   "source": [
    "**Map over Dataframes** Of course, `map` works over Dataframes, too. In this case the callable gets the whole row as a tuple. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  2\n",
       "1  4\n",
       "2  6\n",
       "dtype: int64, length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "def add_unary(tup): \n",
    "    return tup[0]+tup[1]\n",
    "\n",
    "ta.DataFrame({'a': [1,2,3], 'b': [1,2,3]}).map(add_unary , dtype = T.int64)"
   ]
  },
  {
   "source": [
    "**Multi-parameter UDFs**. So far all our user defined functions were unary functions. But `map` can be used for n-ary functions, too: simply specify the set of `columns` you want to pass to the nary function. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  2\n",
       "1  4\n",
       "2  6\n",
       "dtype: int64, length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "def add_binary(a,b):\n",
    "    return a + b\n",
    "\n",
    "ta.DataFrame({'a': [1,2,3], 'b': ['a', 'b', 'c'], 'c':[1,2,3]}).map(add_binary, columns = ['a','c'], dtype = T.int64)"
   ]
  },
  {
   "source": [
    "**Multi-return UDFs.** Functions that return more than one column can be specfied by returning a dataframe  (aka as struct column); providing the  return type is mandatory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  index    quotient    remainder\n",
       "-------  ----------  -----------\n",
       "      0           5            2\n",
       "      1           5            4\n",
       "      2           2            8\n",
       "dtype: Struct([Field('quotient', int64), Field('remainder', int64)]), count: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "ta.DataFrame({'a': [17, 29, 30], 'b': [3,5,11]}).map(divmod, columns= ['a','b'], dtype = T.Struct([T.Field('quotient', T.int64), T.Field('remainder', T.int64)])) "
   ]
  },
  {
   "source": [
    "**UDFs with state**. UDFs need sometimes additional precomputed state. We capture the state in a (data)class and use a method as a delegate:\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  56\n",
       "1  57\n",
       "2  58\n",
       "dtype: int64, length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "def fib(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1 or n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib(n-1) + fib(n-2)\n",
    "    \n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class State:\n",
    "    state: int\n",
    "    def __post_init__(self):\n",
    "        self.state = fib(self.state) \n",
    "    def add_fib(self, x):\n",
    "        return self.state+x\n",
    "\n",
    "m = State(10)\n",
    "ta.Column([1,2,3]).map(m.add_fib)"
   ]
  },
  {
   "source": [
    "TorchArrow requires that only global functions or methods on class instances can be used as user defined functions. Lambdas, which can can capture arbitrary state and are not inspectable, are not supported. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Filter\n",
    "\n",
    "`filter` takes a predicate and returns all those rows for which the predicate holds. Instead of the predicate you can pass an iterable of boolean of the same length as the column. Here are both versions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  1\n",
       "1  1\n",
       "dtype: boolean, length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "\n",
    "ta.Column([1,2,3,4]).filter([True, False, True, False]) == ta.Column([1,2,3,4]).filter(lambda x: x%2==1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "If the predicate is an n-ary function, use the  `columns` argument as we have seen for `map`.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Flatmap\n",
    "\n",
    "`flatmap` combines `map` with `filter`. Each callable can return a list of elements. If that list is empty, flatmap filters, if the returned list is a singleton, flatmap acts like map, if it returns several elements it 'explodes' the input. Here is an example: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  ['I', 'am', 'fine']\n",
       "1  ['I', 'am', 'fine']\n",
       "dtype: List_(string), length: 2, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "def selfish(words):\n",
    "    return [words, words] if len(words)>=1 and words[0] == \"I\" else []\n",
    "\n",
    "sf.flatmap(selfish)"
   ]
  },
  {
   "source": [
    "`flatmap` has all the flexibility of `map`, i.e it can take the `ignore`, `dtype` and `column` arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Reduce\n",
    "`reduce` is just like Python's `reduce`. Here we compute the product of a column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "import operator\n",
    "ta.Column([1,2,3,4]).reduce(operator.mul)"
   ]
  },
  {
   "source": [
    "Even transforms can be expressed with `reduce`.  Here we define `accusum` using the internal(!) column API. The initializer creates and `_Empty` Column, at each iteration we `_append` the new value, once we are done we `_finalize` the column.\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0  1\n",
       "1  3\n",
       "2  6\n",
       "dtype: int64, length: 3, null_count: 0"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "def accusum(col, val):\n",
    "    if len(col) == 0:\n",
    "        col._append(val)\n",
    "    else:\n",
    "        col._append(col[-1] + val)\n",
    "    return col\n",
    "    \n",
    "ta.Column([1,2,3]). reduce(accusum,ta._Empty(T.int64), lambda x: x._finalize())"
   ]
  },
  {
   "source": [
    "Reduce can take the ignore and column arguments as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### UDFs in C++.\n",
    "\n",
    "TorchArrow allows writing any UDF in C++. All you need to do is write the function in C++ and bind it to a name, let's call it `f`, in Python using PyBind. The foreign function `f` behaves now like a normal Python function, in particular it can be passed to any of the functional tools. A function application like `map(f)` now performs the whole computation in C++ and no longer in Python.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Vectorized UDFs (TODO)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Vectorized function leak TorchArrow representation boundaries! So read the following with the big caveat that it can change quickly!\n",
    "\n",
    "Vectorized functions get *n* strongly typed vectors as input and return *m* vectors as output. Validity handling is optional. The following assumes that all data is valid!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def conditional_add(x, y, out):\n",
    "#     for i, (a, e) in enumerate(zip(x, y)):\n",
    "#         if a > 0:\n",
    "#             out[i] = a + e\n",
    "#         else:\n",
    "#             out[i] = a"
   ]
  },
  {
   "source": [
    "This code is perfect for vectorization via Numba. Leveraging Numba will require us to only add some custom attributes. (TODO)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Vectorized functions can be applied using `transform`. We pass a list of data columns and return a typed list of data columns. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ta.transform(conditional_add, incols= ['a','b'], dtypes = [int64]) -- TODO\n",
    "# df.head()"
   ]
  },
  {
   "source": [
    "If you want to pass the underlying vaidity map in and/or out as well, you have to provide it as  incols and out dtypes respectively. The input and output names are called name.data and name.vaidity repectively. The dtype for a validity map is called nullable. So for the folowing transfor, we pass all data and validity masks and return a validity vector as well. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta.transform(conditional_add_with_mask, incols = ['a.data','a.mask', 'b.data', 'b.mask'], dtype = [int64, nullable]]) -- TODO"
   ]
  },
  {
   "source": [
    "Assuming that nulls are handled as bitarrays of 64 bytes each, and that the return must be null if row a's value is > 0, then we can define it like so."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'End of tutorial'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "\"End of tutorial\""
   ]
  },
  {
   "source": [
    "## User defined types (TODO)\n",
    "The notebook *torcharrow_user_defined_types* describe how we can modularly extend torcharrow with new types. As example we will use Tensors. In fact all concrete columns follow the same paradigm. That is w ehave to define a file called X_couln, with two classes and add the class to the int and the two factories. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}